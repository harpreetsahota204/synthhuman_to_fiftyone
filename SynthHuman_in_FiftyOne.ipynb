{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SynthHuman Dataset Analysis in FiftyOne\n",
        "\n",
        "This notebook demonstrates how to load and analyze the SynthHuman dataset using FiftyOne, including:\n",
        "- Loading the dataset from Hugging Face\n",
        "- Computing embeddings and visualizations\n",
        "- Performing similarity analysis\n",
        "- Enriching data with AI-generated labels using Qwen2.5VL\n",
        "\n",
        "## Setup and Installation\n",
        "\n",
        "First, we'll install the required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJVSNAzmlGHa"
      },
      "outputs": [],
      "source": [
        "!pip install fiftyone hf-xet einops qwen_vl_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the SynthHuman Dataset\n",
        "\n",
        "Load the SynthHuman dataset from Hugging Face Hub into FiftyOne format.\n",
        "\n",
        "Note: If you get an error from Hugging Face (like 429, 5xx, etc) then just rerun the cell and pick up from where it left off.\n",
        "\n",
        "Alternatively, you can just pass `max_samples=<some number` to the `load_from_hub` function.\n",
        "\n",
        "Note, there's some issue with downloading the 3D assets from Hugging Face. We're working on it. You can also follow the instructions to download and render the 3D assets locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f191AGQplIxV"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.utils.huggingface as fouh\n",
        "\n",
        "dataset = fouh.load_from_hub(\n",
        "    \"Voxel51/SynthHuman\",\n",
        "    name=\"SynthHuman\",\n",
        "    overwrite=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll just use the RGB images for this analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "synth_human_dataset = dataset.select_group_slices(\"rgb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Embeddings and Visualizations\n",
        "\n",
        "This cell performs several key operations:\n",
        "\n",
        "1. **Registers the C-RADIO v3 model** - A state-of-the-art vision model from NVIDIA Labs\n",
        "\n",
        "2. **Computes embeddings** - Generates 3048-dimensional feature vectors for each image\n",
        "\n",
        "3. **Creates UMAP visualization** - Reduces high-dimensional embeddings to 2D for visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJmAzfI0lh5Z"
      },
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/NVLabs_CRADIOV3\",\n",
        ")\n",
        "\n",
        "# returns a 1D embedding vector, dimensions 3048\n",
        "embedding_model = foz.load_zoo_model(\n",
        "    \"nv_labs/c-radio_v3-h\",\n",
        "    output_type=\"summary\",\n",
        "    feature_format=\"NCHW\",  # \"NCHW\": [Batch, Channels, Height, Width] , or you can use \"NLC\":[Batch, Num_patches, Channels]\n",
        "    install_requirements=True\n",
        ")\n",
        "\n",
        "synth_human_dataset.compute_embeddings(\n",
        "    model=embedding_model,\n",
        "    embeddings_field=\"radio_embeddings\"\n",
        ")\n",
        "\n",
        "# Create UMAP visualization\n",
        "results = fob.compute_visualization(\n",
        "    synth_human_dataset,\n",
        "    method=\"umap\",  # Also supports \"tsne\", \"pca\"\n",
        "    brain_key=\"radio_viz\",\n",
        "    embeddings=\"radio_embeddings\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Similarity Index\n",
        "\n",
        "Build a similarity index using the computed embeddings. This enables fast similarity searches to find images that are visually similar to any given sample:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kehF4bIPpgH2"
      },
      "outputs": [],
      "source": [
        "results = fob.compute_similarity(\n",
        "    synth_human_dataset,\n",
        "    backend=\"sklearn\",  # Fast sklearn backend\n",
        "    brain_key=\"radio_sim\",\n",
        "    embeddings=\"radio_embeddings\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Representativeness Scores\n",
        "\n",
        "Calculate how \"representative\" each sample is of the overall dataset. This helps identify:\n",
        "\n",
        "- **Most representative samples** - Images that best capture the dataset's \n",
        "diversity\n",
        "\n",
        "- **Outliers** - Unusual or unique samples that differ from the norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjndDd8MoY0q"
      },
      "outputs": [],
      "source": [
        "# Compute representativeness scores\n",
        "fob.compute_representativeness(\n",
        "    synth_human_dataset,\n",
        "    representativeness_field=\"radio_represent\",\n",
        "    method=\"cluster-center\",\n",
        "    embeddings=\"radio_embeddings\"\n",
        ")\n",
        "\n",
        "# Find most representative samples\n",
        "representative_view = synth_human_dataset.sort_by(\"radio_represent\", reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting Duplicates and Near-Duplicates\n",
        "\n",
        "Use embeddings to identify duplicate or very similar images in the dataset. \n",
        "\n",
        "This is crucial for:\n",
        "- Data cleaning and deduplication\n",
        "\n",
        "- Understanding dataset quality\n",
        "\n",
        "- Reducing redundancy in training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsJtOK3Ho1xD"
      },
      "outputs": [],
      "source": [
        "# Detect duplicates using embeddings\n",
        "results = fob.compute_uniqueness(\n",
        "    synth_human_dataset,\n",
        "    embeddings=\"radio_embeddings\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Spatial Heatmaps\n",
        "\n",
        "Create spatial attention heatmaps that show which regions of each image the C-RADIO model focuses on. These heatmaps help understand:\n",
        "\n",
        "- What visual features the model considers important\n",
        "\n",
        "- How the model \"sees\" different parts of human images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im03I52ElniC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# returns spatial features which are parsed as a FiftyOne Heatmap\n",
        "\n",
        "spatial_model = foz.load_zoo_model(\n",
        "    \"nv_labs/c-radio_v3-h\",\n",
        "    output_type=\"spatial\",\n",
        "    apply_smoothing=True, # or False\n",
        "    smoothing_sigma=0.51, # used only when apply_smoothing=True\n",
        "    feature_format=\"NCHW\"\n",
        ")\n",
        "\n",
        "synth_human_dataset.apply_model(spatial_model, \"radio_heatmap\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldscbCMwp7BS"
      },
      "source": [
        "# Enrich data using Qwen2.5VL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up Qwen2.5VL Model\n",
        "\n",
        "Register and download the Qwen2.5-VL-3B-Instruct model, a powerful vision-language model that can:\n",
        "\n",
        "- Classify images based on text prompts\n",
        "\n",
        "- Answer questions about image content\n",
        "\n",
        "- Generate detailed descriptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiRcjJJHp7G5"
      },
      "outputs": [],
      "source": [
        "# Register the model source\n",
        "foz.register_zoo_model_source(\"https://github.com/harpreetsahota204/qwen2_5_vl\")\n",
        "\n",
        "# Download the model\n",
        "foz.download_zoo_model(\n",
        "    \"https://github.com/harpreetsahota204/qwen2_5_vl\",\n",
        "    model_name=\"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        ")\n",
        "\n",
        "qwen_model = foz.load_zoo_model(\n",
        "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "    install_requirements=True #if you are using for the first time and need to download reuirement,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classifying Body Poses\n",
        "\n",
        "Use Qwen2.5VL to classify each image into pose categories:\n",
        "\n",
        "- **Face**: Close-up facial shots\n",
        "\n",
        "- **Full Body**: Complete body visible\n",
        "\n",
        "- **Upper Body**: Torso and above visible\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4QIt8LOp7MD"
      },
      "outputs": [],
      "source": [
        "qwen_model.operation = \"classify\"\n",
        "\n",
        "qwen_model.prompt = \"Classify this image into exactly one of the following types: Face, Full Body, Upper Body\"\n",
        "\n",
        "synth_human_dataset.apply_model(qwen_model, label_field=\"pose\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Re-running Pose Classification\n",
        "\n",
        "This appears to be a duplicate of the previous pose classification step. You may want to remove this cell or modify it for a different classification task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfP3PyTtrJS2"
      },
      "outputs": [],
      "source": [
        "qwen_model.prompt = \"Classify this image into exactly one of the following types: Face, Full Body, Upper Body\"\n",
        "\n",
        "synth_human_dataset.apply_model(qwen_model, label_field=\"pose\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Hair Color\n",
        "\n",
        "Extract hair color information from each image using natural language prompts. This adds valuable demographic and appearance metadata to the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0jypBYgrT8i"
      },
      "outputs": [],
      "source": [
        "qwen_model.prompt = \"What is the hair color of this person\"\n",
        "\n",
        "synth_human_dataset.apply_model(qwen_model, label_field=\"hair_color\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Identifying Racial/Ethnic Characteristics\n",
        "\n",
        "Classify the perceived race/ethnicity of individuals in the images. This demographic information can be useful for:\n",
        "- Analyzing dataset diversity and representation\n",
        "- Ensuring balanced training data for fair AI models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsRPzWgLrZtS"
      },
      "outputs": [],
      "source": [
        "qwen_model.prompt = \"What is the race of this person, provide only one response\"\n",
        "\n",
        "synth_human_dataset.apply_model(qwen_model, label_field=\"race\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detecting Facial Expressions\n",
        "\n",
        "Classify facial expressions into seven basic emotion categories:\n",
        "\n",
        "- Happiness, Sadness, Surprise, Fear, Anger, Disgust, Contempt\n",
        "\n",
        "This adds emotional context to the dataset, useful for emotion recognition research and applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8n-b3OMsad7"
      },
      "outputs": [],
      "source": [
        "qwen_model.prompt = \"Classify this image into exactly one of the following facial expressions: Happiness, Sadness, Surprise, Fear, Anger, Disgust, Contempt\"\n",
        "\n",
        "synth_human_dataset.apply_model(qwen_model, label_field=\"facial_expression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Detailed Captions\n",
        "\n",
        "Switch to Visual Question Answering (VQA) mode to generate rich, descriptive captions for each image. These captions include:\n",
        "\n",
        "- Physical appearance details\n",
        "\n",
        "- Facial expressions\n",
        "\n",
        "- Hair styles  \n",
        "\n",
        "- Activities or poses\n",
        "\n",
        "This creates comprehensive textual descriptions that can be used for search, filtering, and understanding dataset content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bME-KZAytW36"
      },
      "outputs": [],
      "source": [
        "qwen_model.operation=\"vqa\"\n",
        "\n",
        "qwen_model.prompt=\"Briefly describe the person in this image, their facial expression, hair style, and what they are doing\"\n",
        "\n",
        "synth_human_dataset.apply_model(qwen_model, label_field=\"caption\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fiftyone",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
